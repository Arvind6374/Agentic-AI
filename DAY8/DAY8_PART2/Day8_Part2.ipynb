{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghWnrtY9yFv_"
      },
      "outputs": [],
      "source": [
        "!pip install -q \\\n",
        "  \"llama-index>=0.11.0\" \\\n",
        "  llama-index-llms-gemini \\\n",
        "  llama-index-embeddings-huggingface \\\n",
        "  llama-index-readers-web \\\n",
        "  sentence-transformers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get API keys from environment variables\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "GEMINI_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "if not GEMINI_API_KEY:\n",
        "    print(\"Warning: GEMINI_API_KEY not found. Gemini model will not run.\")"
      ],
      "metadata": {
        "id": "wvxIHHjEzKy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "gemini_model = \"gemini-2.5-flash-lite\"\n",
        "#model = \"gemini-2.5-flash\"\n",
        "# 1) Gemini for LLM\n",
        "Settings.llm = Gemini(\n",
        "    model=gemini_model\n",
        "    api_key=GEMINI_API_KEY,\n",
        ")\n",
        "Settings.llm.temperature = 0.1\n",
        "\n",
        "# 2) HuggingFace for embeddings (local / free)\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"     # good small RAG embedding model\n",
        ")\n"
      ],
      "metadata": {
        "id": "ncRXNtcQzdfy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "cb8aa685-8967-48f4-c259-95d178c07052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (ipython-input-2628725308.py, line 9)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2628725308.py\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    model=gemini_model\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "\n",
        "urls = [\n",
        "    \"https://developers.llamaindex.ai/python/framework/understanding/rag/\"\n",
        "]\n",
        "\n",
        "reader = SimpleWebPageReader(html_to_text=True)\n",
        "documents = reader.load_data(urls)\n",
        "\n",
        "len(documents), documents[0].metadata"
      ],
      "metadata": {
        "id": "TmNvDyPI0H2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "# Build the vector index using global Settings (Gemini + embeddings)\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Get a basic query engine\n",
        "basic_query_engine = index.as_query_engine(\n",
        "    similarity_top_k=5,   # number of chunks to retrieve\n",
        ")"
      ],
      "metadata": {
        "id": "9R_b3zZd05WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Explain the high-level RAG pipeline described on this page.\"\n",
        "response = basic_query_engine.query(question)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "m8U4ypa61OuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Extra code not part of RAG pipeline\n",
        "\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L2-v2')\n",
        "scores = model.predict([\n",
        "    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n",
        "    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n",
        "])\n",
        "print(scores)\n",
        "# [ 8.510401 -4.860082]"
      ],
      "metadata": {
        "id": "nO7TucGj2ATf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
        "\n",
        "# A lightweight but effective cross-encoder model\n",
        "reranker = SentenceTransformerRerank(\n",
        "    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\",\n",
        "    top_n=3,       # keep only the top 3 chunks after reranking\n",
        ")\n"
      ],
      "metadata": {
        "id": "Rrsm4Pms2LUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rerank_query_engine = index.as_query_engine(\n",
        "    similarity_top_k=10,                # retrieve more candidates\n",
        "    node_postprocessors=[reranker],     # then rerank down to top_n\n",
        ")\n",
        "\n",
        "question = \"How does LlamaIndex split RAG into stages like retrieval and postprocessing?\"\n",
        "response = rerank_query_engine.query(question)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "gUlsGHim2byP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}